OCTOBER 30th Progress Report

Progress in Past 2 Weeks:

Alexis
- Implemented code to compare RL algorithms (Dyna-Q, SARSA) in the ThinIce environment.
- The comparison tracks average reward and success rate for each algorithm, and tests different parameter settings.
- Summarized results to show which algorithm learns faster and performs better.

Next 2 weeks:
- Plan to add more algorithms or extend the environment for deeper comparison.
- Will refine analysis based on feedback.

Jackson
- Added water logic to environment (i.e., floor tile converts to water tile after agent moves off, agent drowns in water)
- Modified reward structure to make agent take longest path in environment
- Modularized code to make a reusable training agent that can be created for different learning algorithms
- Updated environment to properly represent new MDP (new X,Y, W-MASK, AVAIL-ACTION mask states).
- Converted environment and training to reflect how GridWorld in A2 is used (training uses state numbers provided by environment)

Kinjal

Lujain

Trista

Main Accomplishment: 
Established working MDP structure for environment that when train, accomplishes agent goals in proposal.
Created scalable training structure, so that we can begin to add and compare different learning algorithms

Goals for next 2 weeks: 
- Research and implement PPO and DQN learning algorithms (at least)
- Modify environment and training back to having q matrix based on internal state values (rather than generic state numbers)
- Look into (and modify environment and training accordingly), method to generalize training data to unseen levels.