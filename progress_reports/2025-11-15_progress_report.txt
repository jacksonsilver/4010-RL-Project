November 15th Progress Report

Progress in Past 2 Weeks:

Alexis
Worked on the introduction section of the Project Report

Jackson
- Modified MDP to ensure reward is not dependent on previous steps (+1 for newly visited tile that isn't water; otherwise 0)
- Modified state representation to not have w_mask. To respond to env demo feedback - the player could technically walk on water but only as a final terminal step. So in the current algorithms I don't believe this mattered (since we never updated or used the state value of the terminal water states), but this may have mattered in a different algorithm so I kept it in initially.
- Tested and debugged action mask calculations, conversions, etc. Unfortunately it was wrong before but now it's been fixed so we can trust our algorithms
- Implemented decaying buffer for QLearning, going to add it to dynaq.

Kinjal
- Worked briefly on DQN implementation to see if tweaking reward and weights can help the agent to train on certain levels better and solve the maze correctly
- Added logic to refine the policy plot visualized for each level (path does not include tiles not visited, path displays correct action taken), refactored to work without w_mask
- Started work on the Project Report (MDP Specifications section) and expect to work on other sections

Lujain
- Implemented PPO using Mlp (after training - realized it did not work properly)
- Ended up implementing PPO using CNN, works a lot better but only on smaller levels, working on this more for the next week.
- Generation of Graphs of rewards vs epsiode number (show that it converges)
- Renamed all the places files were being generated - so they arent everywhere
- created a "main" file, just to make it look neater (better UI)

Trista
- Changed perform_action function and reward values in attempt to reduce invalid actions and improve performance in DQN 
- Run agent for more time steps to see if more training improved performance or plateaued

Main Accomplishment: 
-  Impleented feedback from env demo
-  Tested and fixed state representations
-  PPO and DynaQ

Goals for next 2 weeks: 
- Research more about optimizing PPO, since it only works on "simpler" levels, maybe changing reward structure, hyperparamaters, certain implementation,etc
- Finish PPO and CNN extension?
- Try to generalize to unseen levels
- Have a good basis for report.
- Better statistical analysis of algorithms.
