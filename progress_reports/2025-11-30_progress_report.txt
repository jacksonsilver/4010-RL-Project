November 30th Progress Report

Progress in Past 2 Weeks:

Alexis
- Modified alogrithm comparison to include Qlearning and added better comparison parameters to better identify which algorithm worked the best for us.
- Investigated an alternative way to find the most optimal algorithm by finding the optimal path and comparing it to the path taken by each of the algorithms but could not get it working.

Jackson
- Added DQL Algorithm
- Added level with hard ice - tile that can be walked on more than once
- Modified MDP to accomodate hard ice - state representation is x, y, and the tile types of the 3x3 grid surrounding player
- Implemented comparison of DQL algorithm with different training values

Kinjal
- Updated the final report to mainly include text on approaches, review of prior work on algorithms, and justification of these approaches

Lujain
- Generated loss graph for training, to make sure loss decreases as training goes on
- Implemented a decaying entropy, so that it converges at the end, and not too early
- Debugging PP
- Tested with different reward structures to see how it effects training

Trista
- Investigate flattening CNN since the only layer that matters is the water/ice layer
- Reduce load on computer with dummy vectorized envs over subprocesses

Main Accomplishment: 

Goals for this week:
- Compare algorithms correctness, and on different training values.
- Create faster testing for DQL (current testing is too slow, use subprocesses)
- Add findings to report


