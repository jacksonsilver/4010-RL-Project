\documentclass{article}
\usepackage[preprint]{neurips} 
\usepackage{graphicx} % Required for inserting images

\title{Reinforcement Learning for Thin Ice} 
\author{
    Alexis Udechukwu\\101225811 \and 
    Lujain Sharafeldin\\101246804\and
    Trista Wang\\101231212\and
    Jackson Silver\\101224148\and
    Kinjal Kamboj\\101227444
}
\date{November 2025}

\begin{document}

\maketitle

\section{Introduction}

\subsection{Problem Focus}
This project will focus on solving the puzzle game "Thin Ice" from Club Penguin using Reinforcement Learning. Thin Ice is a grid-based game where a player navigates a maze to  reach a target tile. A unique aspect of the game that adds more technicality is that each floor tile melts when stepped on, preventing tiles from being revisited and forcing the player to think carefully about the path they take to reach the target tile. 
Our main goal is to train an RL agent to complete the different Thin Ice levels we have provided by reaching the target tile without drowning. Our agent can move in four directions: up, down, left, and right. We have different tiles in the environment: floor tiles (which the agent can walk on), wall tiles (which block movement), water tiles (which floor tiles turn into when stepped on by the agent and cause the agent to drown when the tile is revisited), and a target tile (the goal tile). In addition to reaching the target tile, we make our project more challenging by having another goal for our agent. In Thin Ice, players get the highest score by stepping on every floor tile on their path to the destination. This feature increases the complexity of the task and provides an environment that's better suited for studying reinforcement learning algorithms.

\subsection{Why Should We Care About This Problem?}
Thin Ice has simple rules but has challenges that make it a good environment for studying reinforcement learning. Since each tile melts after being stepped on, the agent must be trained to find paths that reach the target tile without revisiting tiles. This allows the task at hand to become similar to finding a Hamiltonian path,  where each tile is visited once. Working on Thin Ice will give us a chance to see how RL can deal with limited choices, deal with risks and discover paths that paths that cover a whole space, making this project useful both for learning about RL in theory and for applying it to practical applications.

\subsection{MDP Specifications}


\end{document}
